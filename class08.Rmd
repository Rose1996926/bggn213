---
title: 'Class8: Machine Learning'
author: "Rose Li"
date: "4/26/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Kmeans clustering 

Let's try out the **kmeans()** function in R with some makieup data

```{r}
# Generate some example data for clustering
# tmp--> generate30 number with -3 as mean, 30 number with 3 as mean 
tmp <- c(rnorm(30,-3), rnorm(30,3))
# rev-->reverse the number 
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```

Use the kmeans() function setting k to 2 and nstart=20
Inspect/print the results

```{r}
km <- kmeans(x, centers=2, nstart=20)
km
```
[cluster means] denotes the cluster center 
[cluster vector] denotes which of the point belong to which cluster
> Q. How many points are in each cluster?

```{r}
km$size
```

> Q. What ‘component’ of your result object details
      - cluster size?
      - cluster assignment/membership?
      - cluster center?
      
```{r}
km$cluster
table(km$cluster)
km$centers
```
      
Plot x colored by the kmeans cluster assignment and
      add cluster centers as blue points

```{r}
plot(x, col=km$cluster)
points(km$centers, col="blue", pch=18, cex=3)
```

## Hierarchical Clustering 

Here we dont have to spell out K the number of clusters before hand but we do have to give it a distance matrix as input 

```{r}
d <- dist(x)
hc <-hclust(d)
hc 
```

Lets plot the results
```{r}
plot(hc)
abline(h=6, col="red")
cutree(hc, h=6)
```

```{r}
#equivalent to drawing a line at h=3
gp2 <- cutree(hc, k=2)
gp2
```


```{r}
#equivalent to drawing a line at h=3
gp3 <- cutree(hc, k=3)
gp3
```

```{r}
table(gp2)
table(gp3)
```

```{r}
table(gp2, gp3)
```

# Step 1. Generate some example data for clustering
```{r}
x <- rbind(
  matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2),   # c1
  matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
  matrix(c(rnorm(50, mean = 1, sd = 0.3),           # c3
           rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
```
# Step 2. Plot the data without clustering
```{r}
plot(x)
```

# Step 3. Generate colors for known clusters
#         (just so we can compare to hclust results)
```{r}
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```

> Q. Use the dist(), hclust(), plot() and cutree() functions to return 2 and 3 clusters

```{r}
hc <- hclust(dist(x))
hc
plot(hc)
abline(h=1.8, col="red")
```


> Q. How does this compare to your known 'col' groups?

```{r}
gp2 <- cutree(hc, k=2)
gp3 <- cutree(hc, k=3)

table(gp2)
table(gp3)
```

```{r}
table(gp2, gp3)
```

```{r}
plot(x, col=gp3)
```

## Principal Component Analysis (PCA)

we will use the base R **prcomp()** function for PCA today...

Lets get some RNAseq data to play with 
```{r}
mydata <- read.csv("https://tinyurl.com/expression-CSV",
row.names=1)
head(mydata)
```

There are `r nrow(mydata)` genes in this dataset 

```{r}
head(mydata)
```

```{r}
head(t(mydata))
```


```{r}
pca <- prcomp(t(mydata), scale=TRUE)
summary(pca)
```

```{r}
attributes(pca)
```

Lets make our first PCA plot
```{r}
plot(pca$x[,1], pca$x[,2])
```

```{r}
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
```

```{r}
xlab <- paste("PC1 (",pca.var.per[1],"%)", sep="")
ylab <- paste ("PC2 (",pca.var.per[2],"%)", sep="")
           
    
```

```{r}
mycols <- c(rep("red", 5), rep("blue", 5))
```

```{r}
plot(pca$x[,1], pca$x[,2], xlab=xlab, ylab=ylab, col=mycols)
```

```{r}
plot(pca$x[,1], pca$x[,2], xlab=xlab, ylab=ylab, col=mycols)
text(pca$x[,1], pca$x[,2], colnames(mydata))
```

```{r}
plot(pca$x[,1], pca$x[,2])
identify(pca$x[,1], pca$x[,2], colnames(mydata))
```

## PCA of UK Food Data
```{r}
x <- read.csv("UK_foods.csv")
```

```{r}
## Complete the following code to find out how many rows and columns are in x?
dim(x)
```

```{r}
# Note how the minus indexing works
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```

```{r}
dim(x)
```

```{r}
# Alternative 
x <- read.csv("UK_foods.csv", row.names=1)
head(x)
```

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

```{r}
pairs(x, col=rainbow(10), pch=16)
```

```{r}
# Use the prcomp() PCA function 
pca <- prcomp( t(x) )
summary(pca)
```

```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x), col=rainbow(ncol(x)))
```
```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
mycol <- c("orange", "blue", "yellow", "green")
text(pca$x[,1], pca$x[,2], colnames(x), col=mycol)
```
```


```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```

## or the second row here...
```{r}
z <- summary(pca)
z$importance
```

```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```

## Lets focus on PC1 as it accounts for > 90% of variance 
```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

```{r}
biplot(pca)
```


